import json
import time
from abc import ABC, abstractmethod

from typing import Dict, List
import subprocess
import pandas as pd
from autosklearn.classification import AutoSklearnClassifier
import autosklearn.classification
import autosklearn.metrics
import sklearn
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import csv


class SklearnBenchmark:
    def chunks(self,l, n):
        """Yield successive n-sized chunks from l."""
        for i in range(0, len(l), n):
            yield l[i:i + n]

    def _output(self, output_file, result: Dict):
        with open(output_file, 'w') as out:
            out.write(json.dumps(result))
    @abstractmethod
    def _init_model(self, time_limit: int = None):
        pass

    @abstractmethod
    def _best_model(self, model):
        pass

    def _load_data(self,dataset_file, dataset_test_file=None, split=0.75):
            if dataset_test_file is None:
                print(dataset_file)
                data = pd.read_csv(dataset_file, na_values='?')
                train, test = train_test_split(data, train_size=split)
                print(len(train),len(test))
            else:
                train, test = [pd.read_csv(file, na_values='?') for file in [
                    dataset_file, dataset_test_file]]
                data = pd.concat([train, test])

            self.categorical_ = data.select_dtypes(['object']).columns
            lab_encs = {col: LabelEncoder().fit(
                data[col].astype('str')) for col in self.categorical_}

            for col in self.categorical_:
                train[col] = lab_encs[col].transform(train[col].astype('str'))
                test[col] = lab_encs[col].transform(test[col].astype('str'))

            y_col = data.columns[-1]
            return train.drop(y_col, axis=1), train[y_col], test.drop(y_col, axis=1), test[y_col]


    def _evaluate(self,model, X, y,f1):
            predictions=[]
            Xnew=self.chunks(X,10)
            for each in Xnew:                
                predictions.extend(model.predict(each))
            #predictions = model.predict(X)
            return {
                'accuracy': accuracy_score(y, predictions),
                'precision': precision_score(y, predictions, average='weighted'),
                'recall': recall_score(y, predictions, average='weighted'),
                'f1score': f1_score(y, predictions, average=f1)
            }


    def _fit_model(self, model, X, y,metric):
            model.fit(X, y, metric=metric,feat_type=['Categorical' if col in self.categorical_ else 'Numerical' for col in X])
 

    def benchmark(self,inp,dataset_file: str, output_file: str,
            time_limit: int=None,
            dataset_test_file: str=None, split: float=0.75,
            config: List[str]=None,model_name=""):
            X_train, y_train, X_test, y_test = self._load_data(
                dataset_file, dataset_test_file, split)
            if len(set(y_train))>2:
                binary = False
                metric = 'f1_micro'
                f1='micro'
            else:
                binary = True
                metric='f1'
                f1='binary'
            model = self._init_model(inp=inp,time_limit=time_limit)

            result = {}
            scorer = autosklearn.metrics.make_scorer(
            'f1_score',
            sklearn.metrics.f1_score,
            average=f1,
            )
            self._fit_model(model, X_train, y_train,scorer)
            #try:
            result.update(self._evaluate(model, X_test, y_test,f1))
            result['model'] = str(self._best_model(model))
            #except Exception as e:
            #    result['error'] = str(e)
            path="/home/dallal/benchmark/"
            dataf=str(result)
            try:
                
                c = dataf.split('classifier:__choice__')[1].split(',')[0]
            except:
                if "DummyClassifier" in result["model"][0]:
                    c = "DummyClassifier"
                else:
                    c=""
            c=c.replace("'", '')
            c=c.replace(":", '')
            classifier=c.replace(" ", '')

            if len(inp)==15:
                full = True
            else:
                full = False
            csv_file=(path+'extranewskout.tsv')
            dataset_name=    dataset_file.split("/")[-1]
            dataset_name = dataset_name.replace('.csv','')
            dictionary = {"model_name":model_name,"dataset":dataset_name,"classifier": classifier, "accuracy": result["accuracy"], "precision": result["precision"],
                      "recall": result["recall"], "f1score": result["f1score"], "metric":f1,"time_budget": time_limit, "full": full,"methods":inp,"log":dataf}
            log = open("/home/dallal/benchmark/Datasets/log.txt", 'a')
            log.write(dataset_name+"\t"+str(time_limit)+"\n")
            log.write(dataf)
            log.write("\n")
            with open(csv_file, 'a', newline='') as csvfile:
                writer = csv.DictWriter(csvfile, delimiter='\t', fieldnames=dictionary.keys())
                #writer.writeheader()

                
                writer.writerow( {"model_name":model_name,"dataset":dataset_name,"classifier": c, "accuracy": result["accuracy"], "precision": result["precision"],
                      "recall": result["recall"], "f1score": result["f1score"],"metric":f1, "time_budget": time_limit, "full": full,"methods":inp,"log":dataf})

            #self._output(output_file, result)
    

class AutoSklearnBenchmark(SklearnBenchmark):

    def _init_model(self,inp, time_limit: int = None):
        return AutoSklearnClassifier(time_left_for_this_task=(time if time is None else 60 * time_limit),
                                     ml_memory_limit=6144, ensemble_memory_limit=2048,include_estimators=inp)

    def _fit_model(self, model, X, y,metric):

        model.fit(X, y, metric=metric, feat_type=['Categorical' if col in self.categorical_ else 'Numerical' for col in X])
        #model.sprint_statistics()
        #print(model.sprint_statistics())
        #print(model.show_models())
        #print("here",len(model._automl))
        #for i in range(len(model._automl)):
        #    print(model._automl[i].show_models())

    def _best_model(self, model):
        return model.get_models_with_weights()


class AutoSklearnVanillaBenchmark(AutoSklearnBenchmark):

    def _init_model(self, inp,time_limit: int = None):
        return AutoSklearnClassifier(time_left_for_this_task=(time if time is None else 60 * time_limit),
                                     ml_memory_limit=6144, ensemble_memory_limit=2048,
                                     ensemble_size=1, initial_configurations_via_metalearning=0,include_estimators=inp)


class AutoSklearnMetaBenchmark(AutoSklearnBenchmark):

    def _init_model(self,inp, time_limit: int = None):
        return AutoSklearnClassifier(time_left_for_this_task=(time if time is None else 60 * time_limit),
                                     ml_memory_limit=6144, ensemble_memory_limit=2048,
                                     ensemble_size=1,include_estimators=inp)


class AutoSklearnEnsBenchmark(AutoSklearnBenchmark):

    def _init_model(self,inp, time_limit: int = None):
        return AutoSklearnClassifier(time_left_for_this_task=(time if time is None else 60 * time_limit),
                                     ml_memory_limit=6144, ensemble_memory_limit=2048,
                                     initial_configurations_via_metalearning=0,include_estimators=inp)
